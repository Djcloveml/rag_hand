{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import re\n",
    "\n",
    "def ollama_query(context,question):\n",
    "    template = \"\"\"\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    model = OllamaLLM(model=\"qwen2.5:7b\")\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    response = chain.invoke({\"question\": question, \"context\": context})\n",
    "    \n",
    "    # 过滤掉<think>...</think>部分\n",
    "    \n",
    "    # filtered_response = re.sub(r'<think>.*?</think>\\s*', '', response, flags=re.DOTALL)\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain是一个可用的框架，用于构建和运行复杂的语言模型。它允许开发者轻松连接多种模型和数据源，创建高效的问答系统或其他语言处理任务。\\n\\n- LangChain是一个可用的框架，用于构建和运行复杂的语言模型。\\n- 它允许开发者将多种模型和数据源集成在一起。\\n- 这使得用户能够轻松创建如问答系统等高效的语言处理任务。'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_query(\"好用的框架\", \"what is langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def load_documents(file_path:str)->List[Document]:\n",
    "    \n",
    "    # 使用PyPDFLoader加载PDF文件\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    # 加载文档并返回Document对象列表\n",
    "    # 是的，PyPDFLoader.load()返回的documents列表中，每个Document对象代表PDF的一页\n",
    "    # 每个Document包含页面内容和元数据(如页码)\n",
    "    print(\"loading documents...\")\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(documents:List[Document])->List[Document]:\n",
    "    \"\"\"\n",
    "    将文档切分成更小的文本块\n",
    "    \n",
    "    Args:\n",
    "        documents: 包含文档的列表\n",
    "        \n",
    "    Returns:\n",
    "        切分后的文本块列表\n",
    "    \"\"\"\n",
    "    print(\"chunking text...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "# 示例：如何使用chunk_text函数\n",
    "# documents = load_documents(\"your_file.pdf\")\n",
    "# chunks = chunk_text(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading documents...\n",
      "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 0, 'page_label': '1'}, page_content='Sequencer: Deep LSTM for Image Classiﬁcation\\nYuki Tatsunami1,2 Masato Taki1\\n1Rikkyo University, Tokyo, Japan\\n2AnyTech Co., Ltd., Tokyo, Japan\\n{y.tatsunami, taki_m}@rikkyo.ac.jp\\nAbstract\\nIn recent computer vision research, the advent of the Vision Transformer (ViT)\\nhas rapidly revolutionized various architectural design efforts: ViT achieved state-\\nof-the-art image classiﬁcation performance using self-attention found in natural\\nlanguage processing, and MLP-Mixer achieved competitive performance using\\nsimple multi-layer perceptrons. In contrast, several studies have also suggested that\\ncarefully redesigned convolutional neural networks (CNNs) can achieve advanced\\nperformance comparable to ViT without resorting to these new ideas. Against\\nthis background, there is growing interest in what inductive bias is suitable for\\ncomputer vision. Here we propose Sequencer, a novel and competitive architecture\\nalternative to ViT that provides a new perspective on these issues. Unlike ViTs,\\nSequencer models long-range dependencies using LSTMs rather than self-attention\\nlayers. We also propose a two-dimensional version of Sequencer module, where an\\nLSTM is decomposed into vertical and horizontal LSTMs to enhance performance.\\nDespite its simplicity, several experiments demonstrate that Sequencer performs\\nimpressively well: Sequencer2D-L, with 54M parameters, realizes 84.6% top-1\\naccuracy on only ImageNet-1K. Not only that, we show that it has good transfer-\\nability and the robust resolution adaptability on double resolution-band. Our source\\ncode is available at https://github.com/okojoalg/sequencer.\\n1 Introduction\\nSequencer2D-S\\nSequencer2D-M\\nSequencer2D-L\\n0 20 40 60 80 100\\n79\\n80\\n81\\n82\\n83\\n84\\nConvNeXt\\nSwin\\nViP\\nCycleMLP\\nGFNet\\nSequencer(ours)\\nNumber of Parameters(M)\\nImageNet-1k Top-1 Acc.(%)\\nFigure 1: IN-1K top-1 accuracy v.s. model\\nparameters. All models are trained on IN-\\n1K at resolution 2242 from scratch.\\nThe de-facto standard for computer vision has been\\nconvolutional neural networks (CNNs) [39, 64, 22,\\n65, 66, 9, 29, 67]. However, inspired by the many\\nbreakthroughs in natural language processing (NLP)\\nachieved by Transformers [75, 35, 57], applications\\nof Transformers for computer vision are now being\\nactively studied. In particular, Vision Transformer\\n(ViT) [16] is a pure Transformer applied to image\\nrecognition and achieves performance competitive\\nwith CNNs. Various studies triggered by ViT have\\nshown that the state-of-the-art (SOTA) performance\\ncan be achieved for a wide range of vision tasks us-\\ning self-attention alone [79, 48, 73, 47, 15], without\\nconvolution.\\nThe reason for this success is thought to be due to the\\nability of self-attention to model long-range depen-\\ndencies. However, it is still unclear how essential the\\nself-attention is to the effectiveness of Transformers for vision tasks. Indeed, the MLP-Mixer [70]\\nbased only on multi-layer perceptrons (MLPs) is proposed as an appealing alternative to Vision Trans-\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\\narXiv:2205.01972v4  [cs.CV]  12 Jan 2023'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 1, 'page_label': '2'}, page_content='formers (ViTs). In addition, some studies [49, 14] have shown that carefully designed CNNs are still\\ncompetitive enough with Transformers in computer vision. Therefore, identifying which architectural\\ndesigns are inherently effective for computer vision tasks is of great interest for current research [83].\\nThis paper provides a new perspective on this issue by proposing a novel and competitive alternative\\nto these vision architectures.\\nWe propose theSequencer architecture, that uses the long short-term memory (LSTM) [27] rather than\\nthe self-attention for sequence modeling. The macro-architecture design of Sequencer follows ViTs,\\nwhich iteratively applies token mixing and channel mixing, but the self-attention layer is replaced\\nby one based on LSTMs. In particular, Sequencer uses bidirectional LSTM (BiLSTM) [ 63] as a\\nbuilding block. While simple BiLSTM shows a certain level of performance, Sequencer can be further\\nimproved by using ideas similar to Vision Permutator (ViP) [28]. The key idea in ViP is to process the\\nvertical and horizontal axes in parallel. We also introduce two BiLSTMs for top/bottom and left/right\\ndirections in parallel. This modiﬁcation improves the efﬁciency and accuracy of Sequencer because\\nthis structure reduces the length of the sequence and yields a spatially meaningful receptive ﬁeld.\\nWhen pre-trained on ImageNet-1K (IN-1K) dataset, our new attention-free architecture outperforms\\nadvanced architectures such as Swin [ 48] and ConvNeXt [ 49] of comparable size, see Figure 1.\\nIt also outperforms other attention-free and CNN-free architectures such as MLP-Mixer [ 70] and\\nGFNet [61], making Sequencer an attractive new alternative to the self-attention mechanism in vision\\ntasks.\\nThis study also aims to propose novel architecture with practicality by employing LSTM for spatial\\npattern processing. Notably, Sequencer exhibits robust resolution adaptability, which strongly\\nprevents accuracy degradation even when the input’s resolution is increased double during inference.\\nMoreover, ﬁne-tuning Sequencer on high-resolution data can achieve higher accuracy than Swin-B\\n[48] and Sequencer is also useful for semantic segmentation. On peak memory, Sequencer tends to be\\nmore economical than ViTs and recent CNNs for high-resolution input. Although Sequencer requires\\nmore FLOPs than other models due to recursion, the higher resolution improves the relative efﬁciency\\nof peak memory, enhancing the accuracy/cost trade-off at a high-resolution regime. Therefore,\\nSequencer also has attractive properties as a practical image recognition model.\\n2 Related works\\nInspired by the success of Transformers in NLP [ 75, 35, 57, 58, 3, 60], various applications of\\nself-attention have been studied in computer vision. For example, in iGPT [6], an attempt was made\\nto apply autoregressive pre-training with causal self-attention [57] to image classiﬁcation. However,\\ndue to the computational cost of pixel-wise attention, it could only be applied to low-resolution\\nimages, and its ImageNet classiﬁcation performance was signiﬁcantly inferior to the SOTA. ViT [16],\\non the other hand, quickly brought Transformer’s image classiﬁcation performance closer to SOTA\\nwith its idea of applying bidirectional self-attention [35] to image patches rather than pixels. Various\\narchitectural and training improvements [72, 84, 79, 90, 48, 73, 5] have been attempted for ViT [16].\\nIn this paper, we do not improve self-attention itself but propose a completely new module for image\\nclassiﬁcation to replace it.\\nThe extent to which attention-based cross-token communication inherently contributes to ViT’s\\nsuccess is not yet well understood, starting with MLP-Mixer [70], which completely replaced ViT’s\\nself-attention with MLP, various MLP-based architectures [ 71, 46, 28, 69, 68, 13] have achieved\\ncompetitive performance on the ImageNet dataset. We refer to these architectures as global MLPs\\n(GMLPs) because they have global receptive ﬁelds. This series of studies cast doubt on the need\\nfor self-attention. From a practical standpoint, however, these MLP-based models have a drawback:\\nthey need to be ﬁnetuned to cope with ﬂexible input sizes during inference by modifying the shape\\nof their token-mixing MLP blocks. This resolution adaptability problem has been improved in\\nCycleMLP [7], for example, by the idea of realizing a local kernel with a cyclic MLP. There are\\nsimilar ideas such as [ 82, 81, 42, 21] which are collectively referred to as local MLPs (LMLPs).\\nBesides the MLP-based idea, several other interesting self-attention alternatives have been found.\\nGFNet [61] uses Fourier transformation of the tokens and mixes the tokens by global ﬁltering in the\\nfrequency domain. PoolFormer [83], on the other hand, achieved competitive performance with only\\nlocal pooling of tokens, demonstrating that simple local operations are also a suitable alternative.\\nOur proposed Sequencer is a new alternative to self-attention that differs from both of the above, and\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 2, 'page_label': '3'}, page_content='Sequencer is an attempt to realize token mixing in vision architectures using only LSTM. It achieved\\ncompetitive performance with SOTA on the IN-1K benchmark, especially with an architecture that\\ncan ﬂexibly adapt to higher resolution.\\nThe idea of spatial axis decomposition has been used several times in neural architecture in computer\\nvision. For example, SqueezeNeXt [17] decomposes a 3x3 convolution layer into 1x3 and 3x1 convo-\\nlution layers, resulting in a lightweight model. Criss-cross attention [31] reduces memory usage and\\ncomputational complexity by restricting the attention to only vertical and horizontal portions. Current\\narchitectures such as CSwin [15], Couplformer [40], ViP [28], RaftMLP [69], SparseMLP [68], and\\nMorphMLP [86] have included similar ideas to improve efﬁciency and performance.\\nIn the early days of deep learning, there were attempts to use RNNs for image recognition. The\\nearliest study that applied RNNs to image recognition is [19]. The primary difference between our\\nstudy and [19] is that we utilize a usual RNN in place of a 2-multi-dimensional RNN(2MDRNN).\\nThe 2MDRNN requires H+ W sequential operations; The LSTM requires H sequential operations,\\nwhere H and W are height and width, respectively. For subsequent work on image recognition using\\n2MDRNNs, see [ 20, 32, 4, 43]. [4] proposed an architecture in which information is collected from\\nfour directions (upper left, lower left, upper right, and lower right) by RNNs for understanding natural\\nscene images. [43] proposed a novel 2MDRNN for semantic object parsing that integrates global and\\nlocal context information, called LG-LSTM. The overall architecture design is structured to input\\ndeep ConvNet features into the LG-LSTM, unlike Sequencer which stacks LSTMs. ReNet [77] is\\nmost relevant to our work; ReNet [77] uses a 4-way LSTM and non-overlapping patches as input. In\\nthis respect, it is similar to Sequencer. Meanwhile, there are three differences. First, Sequencer is\\nthe ﬁrst MetaFormer [83] realized by adopting LSTM as the token mixing block. Sequencer also\\nadopts a larger patch size than ReNet [ 77]. The beneﬁt of adopting these designs is that we can\\nmodernize LSTM-based vision architectures and fairly compare LSTM-based models with ViT. As a\\nresult, our results provide further evidence for the extremely interesting hypothesis MetaFormer [83].\\nSecond, the way vertical BiLSTMs and horizontal BiLSTMs are connected is different. Our work\\nconnects them in parallel, allowing us to gather vertical and horizontal information simultaneously.\\nOn the other hand, in ReNet [77], the output of the horizontal BiLSTM is used as input to the vertical\\nBiLSTM. Finally, we trained Sequencer on large datasets such as ImageNet, whereas ReNet [77] is\\nlimited to small datasets as MNIST [41], CIFAR-10 [38], and SVHN [54], and has not shown the\\neffectiveness of LSTM for larger datasets. ReSeg [76] applied ReNet to semantic segmentation. RNNs\\nhave been applied not only to image recognition, but also to generative models: PixcelRNN [ 74]\\nis a pixel-channel autoregressive generative model of images using Row RNN, which consists of a\\n1D-convolution and a usual RNN, and Diagonal BiLSTM, which is computationally expensive.\\nIn NLP, attempts have been made to avoid the computational cost of attention by approximating causal\\nself-attention with recurrent neural network (RNN) [34] or replacing it with RNN after training [33].\\nIn particular, in [34], an autoregressive pixel-wise image generation task is experimented with an\\narchitecture where the attentions in iGPT are approximated by RNNs. These studies are speciﬁc\\nto unidirectional Transformers, in contrast to our token-based Sequencer which is the bidirectional\\nanalog of them.\\n3 Method\\nIn this section, we brieﬂy recap the preliminary background on LSTM and further describe the details\\nof the proposed architectures.\\n3.1 Preliminaries: Long short-term memory\\nLSTM [27] is a specialized recurrent neural network (RNN) for modeling long-term dependencies of\\nsequences. Plain LSTM has an input gate it that controls the storage of inputs, a forget gate ft that\\ncontrols the forgetting of the former cell state ct−1 and an output gate ot that controls the cell output\\nht from the current cell state ct. Plain LSTM is formulated as follows:\\nit = σ(Wxixt + Whiht−1 + bi) , ft = σ(Wxf xt + Whf ht−1 + bf ) , (1)\\nct = ft ⊙ct−1 + it ⊙tanh (Wxcxt + Whcht−1 + bc) , ot = σ(Wxoxt + Whoht−1 + bo) , (2)\\nht = ot ⊙tanh(ct), (3)\\nwhere σis the logistic sigmoid function and ⊙is Hadamard product.\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 3, 'page_label': '4'}, page_content='G l o b a l  A v e r a g e  P o o l i n g\\nL i n e a r\\nL a y e r  N o r m\\nP a t c h  E m b e d d i n g\\nI n p u t\\nS e q u e n c e r  B l o c k\\nP a t c h  M e r g i n g\\nS e q u e n c e r  B l o c k\\nP W  L i n e a r\\nS e q u e n c e r  B l o c k\\nP W  L i n e a r\\nS e q u e n c e r  B l o c k\\nC l a s s\\n(a) Sequencer\\nI n p u t\\nC h a n n e l  F u s i o n\\nF o r w a r d  L S T M\\nB a c k w a r d  L S T M\\nF o r w a r d  L S T M\\nB a c k w a r d  L S T M\\nc o n c a tV e r t i c a l\\nB i d i r e c t i o n a l  L S T M\\nH o r i z o n t a l\\nB i d i r e c t i o n a l  L S T M\\nS e q u e n c e i z a t i o n (b) BiLSTM2D layer\\nN o r m\\nN o r m\\nM H  A t t e n t i o n\\nI n p u t\\nC h a n n e l  M L P\\n(c) Transformer\\nblock\\nN o r m\\nN o r m\\nB i L S T M\\nI n p u t\\nC h a n n e l  M L P\\n(d) Vanilla\\nSequencer block\\nN o r m\\nN o r m\\nB i L S T M 2 D\\nI n p u t\\nC h a n n e l  M L P\\n(e) Sequencer2D\\nblock\\nFigure 2: (a) The architecture of Sequencers; (b) The ﬁgure outlines the BiLSTM2D layer, which\\nis the main component of Sequencer2D. (c) Transformer block consist of multi-head attention.\\nIn contrast, (d) Vanilla Sequencer block and (e) Sequencer2D block, utilized on our archtecture,\\ncomposed of BiLSTM or BiLSTM2D instead of multi-head attention.\\nBiLSTM [63] is proﬁtable for sequences where mutual dependencies are expected. A BiLSTM\\nconsists of two plain LSTMs. Let − →x be the input series and ← −x be the rearrangement of − →x in reverse\\norder. − − →hfor and ←−−−hback are the outputs obtained by processing − →x and ← −x with the corresponding\\nLSTMs, respectively. Let −−−→hback be the output ←−−−hback rearranged in the original order, and the output\\nof BiLSTM is obtained as follows:\\n− − →hfor, ←−−−hback = LSTMfor(− →x), LSTMback(← −x), h = concatenate(− − →hfor,−−−→hback) . (4)\\nAssume that both − − →hfor and −−−→hback have the same hidden dimension D, which is hyperparameter of\\nBiLSTM. Accordingly, vector hhas dimension 2D.\\n3.2 Sequencer architecture\\nOverall architecture In the last few years, ViT and its many variants based on self-attention [16,\\n72, 48, 91] have attracted much attention in computer vision. Following these, several works [ 70,\\n71, 46, 28] have been proposed to replace self-attention with MLP. There have also been studies of\\nreplacing self-attention with a hard local induced bias module [7, 83] and with a global ﬁlter [61]\\nusing the fast Fourier transform algorithm (FFT) [10]. This paper continues this trend and attempts to\\nreplace the self-attention layer with LSTM [27]: we propose a new architecture aiming at memory\\nsaving by mixing spatial information with LSTM, which is memory-economical compared to ViT,\\nparameter-saving, and has the ability to learn long-range dependencies.\\nFigure 2a shows the overall structure of Sequencer architecture. Sequencer architecture takes non-\\noverlapping patches as input and projects them onto the feature map. Sequencer block, which is a\\ncore component of Sequencer, consists of the following sub-components: (1) BiLSTM layer can mix\\nspatial information more memory-economically for high-resolution images than Transformer layer\\nand more globally than CNN. (2) MLP for channel-mixing as well as [16, 70]. Sequencer block is\\ncalled Vanilla Sequencer block when plain BiLSTM layers are used as BiLSTM layers as Figure 2d\\nand Sequencer2D block when BiLSTM2D layers are used as Figure 2e. We deﬁne BiLSTM2D layer\\nlater. The output of the last block is sent to the linear classiﬁer via the global average pooling layer,\\nas in most other architectures.\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 4, 'page_label': '5'}, page_content='BiLSTM2D layer We propose the BiLSTM2D layer as a technique to mix 2D spatial information\\nefﬁcaciously. It has two plain BiLSTMs: a vertical BiLSTM and a horizontal one. For an input\\nX ∈RH×W×C, {X:,w,: ∈RH×C}W\\nw=1 is viewed as a set of sequences, where H is the number of\\ntokens in the vertical direction, W is the number of sequences in the horizontal direction, and Cis\\nthe channel dimension. All sequences X:,w,: are input into the vertical BiLSTM with shared weights\\nand hidden dimension D:\\nHver\\n:,w,: = BiLSTM(X:,w,:). (5)\\nIn a very similar manner, {Xh,:,: ∈RW×C}H\\nh=1 is viewed as a set of sequences, and all sequences\\nXh,:,: are input into the horizontal BiLSTM with shared weights and hidden dimension Das well:\\nHhor\\nh,:,: = BiLSTM(Xh,:,:). (6)\\nWe combine {Hver\\n:,w,: ∈RH×2D}W\\nw=1 into Hver ∈RW×H×2D and {Hhor\\nh,:,: ∈RW×2D}H\\nh=1 into\\nHhor ∈RW×H×2D. They are then concatenated and processed point-wisely in a fully-connection\\nlayer. These processes are formulated as follows:\\nH = concatenate(Hver,Hhor), ˆX = FC(H), (7)\\nwhere FC(·) denotes the fully-connected layer with weight W ∈ RC×4D. The PyTorch-like\\npseudocode is shown in Appendix B.1.\\nBiLSTM2D is more memory-economical and throughput-efﬁciency than multi-head-attention of\\nViT for high-resolution input. BiLSTM2D involves (WC + HC)/2 dimensional cell states, while a\\nmulti-head-attention involves h∗(HW)2 dimensional attention map where his a number of heads.\\nThus, as Hand W increase, the memory cost of an attention map increases more rapidly than the cost\\nof a cell state. On throughput, the computational complexity of self-attention is O(W4C), whereas\\nthe computational complexity of BiLSTM is O(WC2) where we assume W = H for simplicity.\\nThere are O(W) sequential operations for BiLSTM2D. Therefore, assuming we use a sufﬁciently\\nefﬁcient LSTM cell implementation, such as ofﬁcial PyTorch LSTMs we are using, the increase of\\nthe complexity of self-attention is much more rapid than BiLSTM2D. It implies a lower throughput\\nof attention compared to BiLSTM2D. See an experiment in Section 4.5.\\nArchitecture variants For comparison between models of different depths consisting of Se-\\nquencer2D blocks, we have prepared three models with different depths: 18, 24, and 36. The\\nnames of the models are Sequencer2D-S, Sequencer2D-M, and Sequencer2D-L, respectively. The\\nhidden dimension is set to D= C/4. Details of these models are provided in Appendix B.2.\\nAs shown in subsection 4.1, these architectures outperform typical models. Interestingly, however,\\nsubsection 4.3 shows that replacing Sequencer2D block with the simpler Vanilla Sequencer block\\nmaintains moderate accuracy. We denote such a model as Vanilla Sequencer. Note that some of the\\nexplicit positional information is lost in the Vanilla Sequencer because the model treats patches as a\\n1D sequence.\\n4 Experiments\\nIn this section, we compare Sequencers with previous studies on the IN-1K benchmark [39]. We also\\ncarry out ablation studies, transfer learning studies, and analysis of the results to demonstrate the\\neffectiveness of Sequencers. We adopt PyTorch [56] and timm [80] library to implement models in\\nthe conduct of all experiments. See Appendix B for more setup details.\\n4.1 Scratch training on IN-1K\\nWe utilize IN-1K [39], which has 1000 classes and contains 1,281,167 training images and 50,000\\nvalidation images. We adopt AdamW optimizer [50]. Following the previous study [72], we adopt\\nthe base learning rate batch size\\n512 ×5 ×10−4. The batch sizes for Sequencer2D-S, Sequencer2D-M,\\nand Sequencer2D-L are 2048, 1536, and 1024, respectively. As a regularization method, stochastic\\ndepth [30] and label smoothing [ 66] are employed. As data augmentation methods, mixup [ 87],\\ncutout [12], cutmix [85], random erasing [88], and randaugment [11] are applied.\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 5, 'page_label': '6'}, page_content='Table 1: The table shows the top-1 accuracy when trained on IN-1K, comparing our model with\\nother similar scale representative models. Training and inference throughput and their peak memory\\nwere measured with 16 images per batch on a single V100 GPU. The left sides of the slashes are\\nvalues during training, and the right sides of the slashes are values during inference. Fine-tuned\\nmodels marked with \"↑\". Note Sequencer2D-L↑are compared to Swin-B↑and ConvNeXt-B↑with\\nmore parameters since Swin and ConvNeXt have not ﬁne-tuned models of similar parameters with\\nSequencer2D-L↑in the original papers.\\nModel Family Res. #Param. FLOPs Throughput Peak Mem. Top-1 Pre FT Top-1\\n(image/s) (MB) Acc.(%) Acc.(%)\\nTraining from scratch\\nRegNetY-4GF [59] CNN 2242 21M 4.0G 228/823 1136/225 80.0 non-ﬁne-tune\\nConvNeXt-T [49] CNN 2242 29M 4.5G 337/1124 1418/248 82.1 as above\\nDeiT-S [72] Trans. 2242 22M 4.6G 480/1569 1195/180 79.9 as above\\nSwin-T [48] Trans. 2242 28M 4.5G 268/894 1613/308 81.2 as above\\nViP-S/7 [28] GMLP 2242 25M 6.9G 214/702 1587/195 81.5 as above\\nCycleMLP-B2 [7] LMLP 2242 27M 3.9G 158/586 1357/234 81.6 as above\\nPoolFormer-S24 [83] LMLP 2242 21M 3.6G 313/988 1461/183 80.3 as above\\nSequencer2D-S Seq. 2242 28M 8.4G 110/347 1799/196 82.3 as above\\nRegNetY-8GF [59] CNN 2242 39M 8.0G 211/751 1776/333 81.7 as above\\nT2T-ViTt-19 [84] Trans. 2242 39M 9.8G 197/654 3520/1140 82.2 as above\\nCycleMLP-B3 [7] LMLP 2242 38M 6.9G 100/367 2326/287 82.6 as above\\nPoolFormer-S36 [83] LMLP 2242 31M 5.2G 213/673 2187/220 81.4 as above\\nGFNet-H-S [61] FFT 2242 32M 4.5G 227/755 1740/282 81.5 as above\\nSequencer2D-M Seq. 2242 38M 11.1G 83/270 2311/244 82.8 as above\\nRegNetY-12GF [59] CNN 2242 46M 12.0G 199/695 2181/440 82.4 as above\\nConvNeXt-S [49] CNN 2242 50M 8.7G 212/717 2265/341 83.1 as above\\nSwin-S [48] Trans. 2242 50M 8.7G 165/566 2635/390 83.2 as above\\nMixer-B/16 [70] GMLP 2242 59M 12.7G 338/1011 1864/407 76.4 as above\\nViP-M/7 [28] GMLP 2242 55M 16.3G 130/395 3095/396 82.7 as above\\nCycleMLP-B4 [7] LMLP 2242 52M 10.1G 70/259 3272/338 83.0 as above\\nPoolFormer-M36 [83] LMLP 2242 56M 9.1G 171/496 3191/368 82.1 as above\\nGFNet-H-B [61] FFT 2242 54M 8.4G 144/482 2776/367 82.9 as above\\nSequencer2D-L Seq. 2242 54M 16.6G 54/173 3516/322 83.4 as above\\nFine-tuning\\nConvNeXt-B↑[49] CNN 3842 89M 45.1G 78/234 7329/870 85.1(+1.3) 83.8\\nSwin-B↑[48] Trans. 3842 88M 47.1G 54/156 12933/1532 84.5(+1.0) 83.5\\nGFNet-B↑[61] FFT 3842 47M 23.2G 137/390 3710/416 82.1(+0.8) 82.9\\nSequencer2D-L↑ Seq. 3922 54M 50.7G 26/84 9062/481 84.6(+1.2) 83.4\\nTable 1 shows the results that are comparing the proposed models to others with a comparable number\\nof parameters to our models, including models with local and global receptive ﬁelds such as CNNs,\\nViTs, and MLP-based and FFT-based models. Sequencers has the disadvantage that its throughput\\nis slower than other models because it uses RNNs. In the scratch training on IN-1K, however, they\\noutperform these recent comparative models in accuracy across their parameter bands. In particular,\\nSeqeuncer2D-L is competitive with recently discussed models with comparable parameters such as\\nConvNeXt-S [49] and Swin-S [48], with accuracy outperformance of 0.3% and 0.2%, respectively.\\nTable 1 demonstrates that Sequencer’s throughput is not good. The training throughput is about\\nthree times the inference throughput for all these models. Compared to other models, both measured\\ninference and training time are not good.\\n4.2 Fine-tuning on IN-1K\\nIn this ﬁne-tuning study, Sequencer2D-L pre-trained on IN-1K at 224 2 resolution is ﬁne-tuned\\non IN-1K at 392 2 resolution. We compare it with the other models ﬁne-tuned on IN-1K at 384 2\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 6, 'page_label': '7'}, page_content='Table 2: Sequencer ablation experiments . We adopt Sequencer2D-S variant for these ablation\\nstudies. C1 denotes vertical BiLSTM,C2 denotes horizontal BiLSTM, andC3 denotes channel fusion\\ncomponent. When vertical BiLSTM only, horizontal BiLSTM only or unidirectional BiLSTM2D, its\\nhidden dimension needs to be doubled from the original setting because it compensates the output\\ndimension for the excluded LSTM and matches the dimensions.\\n(a) Components\\nC1 C2 C3 Acc.\\n✓ 75.6\\n✓ 75.0\\n✓ ✓ 81.6\\n✓ ✓ ✓ 82.3\\n(b) LSTM Direction\\nBidirectional Acc.\\n79.7\\n✓ 82.3\\n(c) Vanilla Sequencer\\nModel #Params. FLOPs Acc.\\nVSequencer-S 33M 8.4G 78.0\\nVSequencer(H)-S 28M 8.4G 78.8\\nVSequencer(PE)-S 33M 8.4G 78.1\\nSequencer2D-S 28M 8.4G 82.3\\n(d) Hidden dimension\\nHidden dim. ratio #Params. FLOPs Acc.\\n1x 28M 8.4G 82.3\\n2x 45M 13.9G 82.6\\n(e) Various RNNs\\nModel #Params. FLOPs Acc.\\nRNN-Sequencer2D 19M 5.8G 80.6\\nGRU-Sequencer2D 25M 7.5G 82.3\\nSeqeucer2D-S 28M 8.4G 82.3\\nresolution. Since 392 is divisible by 14, the input at this resolution can be split into patches without\\npadding. However, note that this is not the case with a resolution of 3842.\\nAs Table 1 indicates, even when higher-resolution Sequencer is ﬁne-tuned, it is competitive with the\\nlatest models such as ConvNeXt [49], Swin [48], and GFNet [61].\\n4.3 Ablation studies\\nThis subsection presents ablation studies based on Sequencer2D-S for further understanding of\\nSequencer. We seek to clarify the effectiveness and validity of the Sequencers architecture in terms\\nof the importance of each component, bidirectional necessaries, setting of the hidden dimension, and\\nthe comparison with simple BiLSTM.\\nWe show where and how relevant the components of BiLSTM2D are: The BiLSTM2D is composed\\nof vertical BiLSTM, horizontal BiLSTM, and channel fusion elements. We want to see the validity of\\nvertical BiLSTM, horizontal BiLSTM, and channel fusion. For this purpose, we examine the removal\\nof channel fusion and vertical or horizontal BiLSTM. Table 2a shows the results. Removing channel\\nfusion shows that the performance degrades from 82.3% to 81.6%. Furthermore, the additional\\nremoval of vertical or horizontal BiLSTM exposes a 6.0% or 6.6% performance drop, respectively.\\nHence, each component discussed here is necessary for Sequencer2D.\\nWe show that the bidirectionality for BiLSTM2D is important for Sequencer. We compare\\nSequencer2D-S with a version that replaces the vertical and horizontal BiLSTMs with vertical\\nand horizontal unidirectional LSTMs. Table 2b shows that the unidirectional model is 2.6% less\\naccurate than the bidirectional model. This result attests to the signiﬁcance of using not unidirectional\\nLSTM but BiLSTM.\\nIt is important to set the hidden dimension of LSTM to a reasonable size. As described in subsection\\n3.2, Sequencer2D sets the hidden dimension Dof BiLSTM to D= C/4, but this is not necessary if\\nthe model has channel fusions. Table 2d compares Sequencer2D-S with the model with increased\\nD. Although accuracy is 0.3% improved, FLOPs increase by 65%, and the number of parameters\\nincreases by 60%. Namely, the accuracy has not improved for the increase in FLOPs. Moreover, the\\nincrease in dimension causes overﬁtting, which is discussed in Appendix C.3.\\nVanilla Sequencer can also achieve accuracy that outperforms MLP-Mixer [70], but is not as accu-\\nrate as Sequencer2D. Following experimental result supports the claim. We experiment with the\\nSequencer2D-S variants, where Vanilla Sequencer blocks replace the Sequencer2D blocks, called\\nVSequencer-S(H), with incomplete positional information. In addition, we experiment with a variant\\nof VSequencer-S(H) without the hierarchical structure, which we call VSequencer-S. VSequencer-\\nS(PE) is VSequencer-S using ViTs-style learned positional embedding (PE) [ 16]. Table 2c indicates\\neffectiveness for combination of LSTM and ViTs-like architecture. Surprisingly, even with Vanilla\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 7, 'page_label': '8'}, page_content='Table 3: Left. Results on transfer learning. We transfer models trained on IN-1K to datasets from\\ndifferent domains. Sequencers use 2242 resolution images, while ViT-B/16 and EfﬁcientNet-B7 work\\non higher resolution, see Res. column. Right. Semantic segmentation results on ADE20K [89]. All\\nmodels are Semantic FPN [36] based. We show mIoU for the ADE20k validation set.\\nModel Res. #Pr. FLOPs CF10 CF100 Flowers Cars\\nResNet50 [22] 2242 26M 4.1G - - 96.2 90.0\\nEN-B7 [67] 6002 26M 37.0G 98.9 91.7 98.8 94.7\\nViT-B/16 [16] 3842 86M 55.4G 98.1 87.1 89.5 -\\nDeiT-B [72] 2242 86M 17.5G 99.1 90.8 98.4 92.1\\nCaiT-S-36 [73] 2242 68M 13.9G 99.2 92.2 98.8 93.5\\nResMLP-24 [71] 2242 30M 6.0G 98.7 89.5 97.9 89.5\\nGFNet-H-B [61] 2242 54M 8.6G 99.0 90.3 98.8 93.2\\nSequencer2D-S 2242 28M 8.4G 99.0 90.6 98.2 93.1\\nSequencer2D-M 2242 38M 11.1G 99.1 90.8 98.2 93.3\\nSequencer2D-L 2242 54M 16.6G 99.1 91.2 98.6 93.1\\nModel #Pr. mIoU\\nPVT-Small [79] 28M 39.8\\nPoolFormer-S24 [83] 23M 40.3\\nSequencer2D-S 32M 46.1\\nPVT-Medium [79] 48M 41.6\\nPoolFormer-S36[83] 35M 42.0\\nSequencer2D-M 42M 47.3\\nPVT-Large [79] 65M 42.1\\nPoolFormer-M36 [83] 60M 42.4\\nSequencer2D-L 58M 48.6\\nSequencer and Vanilla Sequencer(H) without PE, the performance reduction from Sequencer2D-S is\\nonly 4.3% and 3.5%, respectively. According to these results, there is no doubt that Vanilla Sequencer\\nusing BiLSTMs is signiﬁcant enough, although not as accurate as Sequencer2D.\\nAll LSTMs in the BiLSTM2D layer can be replaced with other recurrent networks such as gated\\nrecurrent units (GRUs) [8] or tanh-RNNs to deﬁne BiGRU2D layer or BiRNN2D layer. We also\\ntrained these models on IN-1K, so see Table 2e for the results. The table suggests that all of these\\nvariants, including RNN-cell, work well. Also, tanh-RNN performs slightly worse than others,\\nprobably due to its lower ability to model long-range dependence.\\n4.4 Transfer learning and semantic segmentation\\nSequencers perform well on IN-1K, and they have good transferability. In other words, they have\\nsatisfactory generalization performance for a new domain, which is shown below. We utilize the\\ncommonly used CIFAR-10 [38], CIFAR-100 [38], Flowers-102 [55], and Stanford Cars [ 37] for\\nthis experiment. See the references and Appendix B.4 for details on the datasets. The results of the\\nproposed model and the results in previous studies of models with comparable capacity are presented\\nin Table 3. In particular, Sequencer2D-L achieves results that are competitive with CaiT-S-36 [73]\\nand EfﬁcientNet-B7 [67].\\nWe experiment for semantic segmentation on ADE20K[89] dataset. See Appendix C.4 for details on\\nthe setup. Sequencer outperforms PVT [79] and PoolFormer [83] with similar parameters; compared\\nto PoolFormer, mIoU is about 6 pts higher.\\nWe have investigated a commonly object detection model with Sequencer as the backbone. Its\\nperformance is not much different from the case of ResNet [22] backbone. Its improvement is the\\nfuture work. See Appendix C.5.\\n4.5 Analysis and visualization\\nIn this subsection, we investigate the properties of Sequencer in terms of resolution adaptability and\\nefﬁciency. Furthermore, effective receptive ﬁeld (ERF) [51] and visualization of the hidden states\\nprovides insight into the question of how Sequencer recognizes images.\\nOne of the attractive properties of Sequencer is its ﬂexible adaptability to the resolution, with minimal\\nimpact on accuracy even when the resolution of the input image is varied from one-half to twice.\\nIn comparison, architectures like MLP-Mixer [ 70] have a ﬁxed input resolution, and GFNet [ 61]\\nrequires interpolation of weights in the Fourier domain when inputting images with a resolution\\ndifferent from training images. We evaluate the resolution adaptability of models comparatively by\\ninputting different resolution images to each model, without ﬁne-tuning, with pre-trained weights\\non IN-1K at the resolution of 224 2. Figure 3a compares absolute top-1 accuracy on IN-1K, and\\nFigure 3b compares relative one to the input image with the resolution of 2242. By increasing the\\nresolution by 28 for Sequencer2D-S and by 32 for other models, we avoid padding and prevent\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 8, 'page_label': '9'}, page_content='the effect of padding on accuracy. Compared to DeiT-S [ 72], GFNet-S [ 61], CycleMLP-B2 [ 7],\\nand ConvNeXt-T [49], Sequencer-S’s performance is more sustainable. The relative accuracy is\\nconsistently better than ConvNeXt [49], which is inﬂuential in the lower-resolution band, and, at\\n4482 resolution, 0.6% higher than CycleMLP [7], which is inﬂuential in the double-resolution band.\\nIt is noteworthy that Sequencer continues to maintain high accuracy on double resolution.\\nThe higher the input resolution, the higher memory-efﬁciency and throughput of Sequencers when\\ncompared to DeiT [72]. Figure 3 shows the efﬁciency of Sequencer2D-S when compared to DeiT-S\\nand ConvNeXt-T [49]. Memory consumption increases rapidly in DeiT-S and ConvNeXt-T with\\nincreasing input resolution, but more gradual increase in Sequencer2D-S. The result strongly implies\\nthat it has more practical potential as the resolution increases than the ViTs. At a resolution of 2242,\\nit is behind DeiT in throughput, but it stands ahead of DeiT when images with a resolution of 8962\\nare input.\\n128 224 320 41665\\n70\\n75\\n80\\n85\\nDeiT-S\\nGFNet-S\\nCycleM LP-B2\\nConvNeXt-T\\nSequencer2D-S(ours)\\nResolution\\nImageNet-1k Top-1 Acc.(%)\\n(a) Absolute top-1 acc.\\n128 224 320 416\\n−14\\n−12\\n−10\\n−8\\n−6\\n−4\\n−2\\n0\\n2\\nDeiT-S\\nGFNet-S\\nCy cleM LP-B2\\nConv NeXt-T\\nSequencer2 D-S(ours)\\nR e solution\\nΔ I m a g e N e t-1k T op-1 A c c .(%) (b) Relative top-1 acc. to 2242 res.\\n224 448 672 896 1120 1344 1568 1792 2016 22400\\n5k\\n10k\\n15k DeiT-S\\nConv Nex t-T\\nSequencer2 D-S\\nR e solution\\nP e a k M e m oly ( M B )\\n(c) GPU peak memory\\n224 448 672 896 1120 1344 1568 1792 2016 22401\\n2\\n5\\n10\\n2\\n5\\n100\\n2\\n5\\n1000\\n2\\n5\\n10k\\nDeiT- S\\nConv Nex t- T\\nSequencer2 D- S\\nRe s o l uti o n\\nTh r o ug h p ut(i m a g e /s e c ) (d) Throughput\\nFigure 3: Top. Resolution adaptability. Every model is trained at\\n2242 resolution and evaluated at various resolutions with no ﬁne-\\ntuning. Bottom. Comparisons among Sequencer2D-S, DeiT-S [72],\\nand ConvNeXt-T [49] in (c) GPU peak memory and (d) throughput\\nfor different input image resolutions. Measured for each increment\\nof 2242 resolution, points not plotted are when GPU memory is ex-\\nhausted. The measurements are founded on a batch size of 16 and a\\nsingle V100.\\nFigure 4: Part of states\\nof the last BiLSTM2D\\nlayer in the Sequencer\\nblock of stage 1. From\\ntop to bottom: out-\\nputs of ver-LSTM, hor-\\nLSTM, and ch-fusions\\nand original images.\\nIn general, CNNs have localized, layer-by-layer expanding receptive ﬁelds, and ViTs without shifted\\nwindows capture global dependencies, working the self-attention mechanism. In contrast, in the\\ncase of Sequencer, it is not clear how information is processed in Sequencer block. We calculated\\nERF [51] for ResNet-50 [ 22], DeiT-S [72], and Sequencer2D-S as shown in Figure 5. ERFs of\\nSequencer2D-S form a cruciform shape in all layers. The trend distinguishes it from well-known\\nmodels such as DeiT-S and ResNet-50. More remarkably, in shallow layers, Sequencer2D-S has a\\nwider ERF than ResNet-50, although not as wide as DeiT. This observation conﬁrms that LSTMs in\\nSequencer can model long-term dependencies as expected and that Sequencer recognizes sufﬁciently\\nlong vertical or horizontal regions. Thus, it can be argued that Sequencer recognizes an image in a\\nvery different way than CNNs or ViTs. For more details on ERF and additional visualization, see\\nAppendix D.\\nMoreover we also visualized a hidden state of vertical and horizontal BiLSTM, and a feature map\\nafter channel fusion, and the results are visualized in Figure 4. It demonstrates that our Sequencer\\nhas the hidden states interact with each other over the vertical and horizontal directions. The closer\\ntokens are in position, the stronger their interaction tends to be; the farther tokens are in position, the\\nless their interaction tends to be.\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 9, 'page_label': '10'}, page_content='(a) RN50/ﬁrst\\n (b) DeiTS/ﬁrst\\n (c) SeqS/ﬁrst\\n (d) RN50/last\\n (e) DeiTS/last\\n (f) SeqS/last\\nFigure 5: The visualizations are the ERFs of Sequencer2D-S and comparative models such as ResNet-\\n50 and DeiT-S. The left of the slash denotes the model name, and the right of the slash denotes the\\nlocation of the block of output used to generate the ERFs. The ERFs are rescaled from 0 to 1. The\\nbrighter and more inﬂuential the region is, the closer to 1, and the darker, the closer to 0.\\n5 Conclusions\\nWe propose a novel and simple architecture that leverages LSTM for computer vision. It is demon-\\nstrated that new modeling with LSTM instead of the self-attention layer can achieve competitive\\nperformance with current state-of-the-art models. Our experiments show that Sequencer has a good\\nmemory-resource/accuracy and parameter/accuracy tradeoffs, comparable to the main existing meth-\\nods. Despite the impact of recursion on throughput, we have demonstrated beneﬁts over it. We\\nbelieve that these results raise a number of interesting issues. Improving Sequencer’s poor throughput\\nis one example. Moreover, we expect that investigating the internal mechanisms of our model using\\nmethods other than ERF will further our understanding of how this architecture works. In addition,\\nit would be important to analyze in more detail the features learned by Sequencer in comparison to\\nother architectures. We hope this will lead to a better understanding of the role of various inductive\\nbiases in computer vision. Furthermore, we expect that our results trigger further study beyond the\\ndomain or research area. Especially, it would be a very interesting open question to see if such\\na design works with time-series data in vision such as video or in a multi-modal problem setting\\ncombined with another modality such as video with audio.\\nAcknowledgments and Disclosure of Funding\\nOur colleagues at AnyTech Co., Ltd. provided valuable comments on the early versions and encour-\\nagement. We thank them for their cooperation. In particular, We thank Atsushi Fukuda for organizing\\ndiscussion opportunities. We also thank people who support us, belonging to Graduate School of\\nArtiﬁcial Intelligence and Science, Rikkyo University.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. In NeurIPS, 2016.\\n[2] Lucas Beyer, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we\\ndone with imagenet? arXiv preprint arXiv:2006.07159, 2020.\\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\\nIn NeurIPS, volume 33, pages 1877–1901, 2020.\\n[4] Wonmin Byeon, Thomas M Breuel, Federico Raue, and Marcus Liwicki. Scene labeling with lstm recurrent\\nneural networks. In CVPR, pages 3547–3555, 2015.\\n[5] Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision\\ntransformer for image classiﬁcation. In ICCV, pages 357–366, 2021.\\n[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\\nGenerative pretraining from pixels. In ICML, pages 1691–1703. PMLR, 2020.\\n[7] Shoufa Chen, Enze Xie, Chongjian GE, Runjian Chen, Ding Liang, and Ping Luo. CycleMLP: A MLP-like\\narchitecture for dense prediction. In ICLR, 2022.\\n[8] Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of\\nneural machine translation: Encoder–decoder approaches. In Proceedings of SSST-8, Eighth Workshop on\\nSyntax, Semantics and Structure in Statistical Translation, pages 103–111, 2014.\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 10, 'page_label': '11'}, page_content='[9] François Chollet. Xception: Deep learning with depthwise separable convolutions. In CVPR, pages\\n1251–1258, 2017.\\n[10] James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier series.\\nMathematics of computation, 19(90):297–301, 1965.\\n[11] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. RandAugment: Practical automated data\\naugmentation with a reduced search space. In CVPRW, pages 702–703, 2020.\\n[12] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with\\ncutout. arXiv preprint arXiv:1708.04552, 2017.\\n[13] Xiaohan Ding, Honghao Chen, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Repmlpnet: Hierarchi-\\ncal vision mlp with re-parameterized locality. In CVPR, 2022.\\n[14] Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong Han, Guiguang Ding, and Jian Sun. Scaling up\\nyour kernels to 31x31: Revisiting large kernel design in cnns. In CVPR, 2022.\\n[15] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and\\nBaining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In\\nCVPR, 2022.\\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\\n16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\\n[17] Amir Gholami, Kiseok Kwon, Bichen Wu, Zizheng Tai, Xiangyu Yue, Peter Jin, Sicheng Zhao, and Kurt\\nKeutzer. Squeezenext: Hardware-aware neural network design. In CVPRW, pages 1638–1647, 2018.\\n[18] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.\\nIn ICLR, 2015.\\n[19] Alex Graves, Santiago Fernández, and Jürgen Schmidhuber. Multi-dimensional recurrent neural networks.\\nIn International conference on artiﬁcial neural networks, pages 549–558. Springer, 2007.\\n[20] Alex Graves and Jürgen Schmidhuber. Ofﬂine handwriting recognition with multidimensional recurrent\\nneural networks. In NeurIPS, volume 21, 2008.\\n[21] Jianyuan Guo, Yehui Tang, Kai Han, Xinghao Chen, Han Wu, Chao Xu, Chang Xu, and Yunhe Wang.\\nHire-mlp: Vision mlp via hierarchical rearrangement. arXiv preprint arXiv:2108.13341, 2021.\\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\\nIn CVPR, pages 770–778, 2016.\\n[23] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,\\nTyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of\\nout-of-distribution generalization. In ICCV, pages 8340–8349, 2021.\\n[24] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions\\nand perturbations. In ICLR, 2018.\\n[25] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint arXiv:1606.08415,\\n2016.\\n[26] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial\\nexamples. In CVPR, pages 15262–15271, 2021.\\n[27] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,\\n1997.\\n[28] Qibin Hou, Zihang Jiang, Li Yuan, Ming-Ming Cheng, Shuicheng Yan, and Jiashi Feng. Vision permutator:\\nA permutable mlp-like architecture for visual recognition. IEEE TPAMI, 2022.\\n[29] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected\\nconvolutional networks. In CVPR, pages 4700–4708, 2017.\\n[30] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic\\ndepth. In ECCV, pages 646–661, 2016.\\n[31] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:\\nCriss-cross attention for semantic segmentation. In ICCV, pages 603–612, 2019.\\n[32] Nal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid long short-term memory. In In Proceedings of\\nthe IEEE Workshop on Spoken Language Technology, 2015.\\n[33] Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu\\nChen, and Noah A Smith. Finetuning pretrained transformers into rnns. In EMNLP, pages 10630–10643,\\n2021.\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 11, 'page_label': '12'}, page_content='[34] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast\\nautoregressive transformers with linear attention. In ICML, pages 5156–5165. PMLR, 2020.\\n[35] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. In NAACL-HLT, pages 4171–4186, 2019.\\n[36] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dollár. Panoptic feature pyramid networks. In\\nCVPR, pages 6399–6408, 2019.\\n[37] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3D object representations for ﬁne-grained\\ncategorization. In ICCVW, pages 554–561, 2013.\\n[38] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical\\nreport, University of Toronto, 2009.\\n[39] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional\\nneural networks. In NeurIPS, volume 25, pages 1097–1105, 2012.\\n[40] Hai Lan, Xihao Wang, and Xian Wei. Couplformer: Rethinking vision transformer with coupling attention\\nmap. arXiv preprint arXiv:2112.05425, 2021.\\n[41] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\\n[42] Dongze Lian, Zehao Yu, Xing Sun, and Shenghua Gao. As-mlp: An axial shifted mlp architecture for\\nvision. In ICLR, 2022.\\n[43] Xiaodan Liang, Xiaohui Shen, Donglai Xiang, Jiashi Feng, Liang Lin, and Shuicheng Yan. Semantic\\nobject parsing with local-global long short-term memory. In CVPR, pages 3185–3193, 2016.\\n[44] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object\\ndetection. In ICCV, pages 2980–2988, 2017.\\n[45] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740–755, 2014.\\n[46] Hanxiao Liu, Zihang Dai, David So, and Quoc Le. Pay attention to mlps. In NeurIPS, volume 34, 2021.\\n[47] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang,\\nLi Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In CVPR, 2022.\\n[48] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\\ntransformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.\\n[49] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A\\nconvnet for the 2020s. In CVPR, 2022.\\n[50] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\\n[51] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive ﬁeld in\\ndeep convolutional neural networks. In NeurIPS, volume 29, 2016.\\n[52] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards\\ndeep learning models resistant to adversarial attacks. In ICLR, 2018.\\n[53] Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye, Yuan He, and Hui Xue.\\nTowards robust vision transformer. arXiv preprint arXiv:2105.07926, 2021.\\n[54] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits\\nin natural images with unsupervised feature learning. In NeurIPS Workshop, 2011.\\n[55] Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number of\\nclasses. In ICVGIP, pages 722–729, 2008.\\n[56] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\\nZeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep\\nlearning library. In NeurIPS, volume 32, 2019.\\n[57] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding\\nby generative pre-training. Technical report, OpenAI, 2018.\\n[58] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n[59] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollár. Designing network\\ndesign spaces. In CVPR, pages 10428–10436, 2020.\\n[60] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.\\nJMLR, 21:1–67, 2020.\\n12'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 12, 'page_label': '13'}, page_content='[61] Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and Jie Zhou. Global ﬁlter networks for image\\nclassiﬁcation. In NeurIPS, volume 34, 2021.\\n[62] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers\\ngeneralize to imagenet? In ICML, pages 5389–5400. PMLR, 2019.\\n[63] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. IEEE TSP, 45(11):2673–\\n2681, 1997.\\n[64] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-\\ntion. In ICLR, 2015.\\n[65] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru\\nErhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, pages\\n1–9, 2015.\\n[66] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the\\ninception architecture for computer vision. In CVPR, pages 2818–2826, 2016.\\n[67] Mingxing Tan and Quoc Le. EfﬁcientNet: Rethinking model scaling for convolutional neural networks. In\\nICML, pages 6105–6114, 2019.\\n[68] Chuanxin Tang, Yucheng Zhao, Guangting Wang, Chong Luo, Wenxuan Xie, and Wenjun Zeng. Sparse\\nmlp for image recognition: Is self-attention really necessary? arXiv preprint arXiv:2109.05422, 2021.\\n[69] Yuki Tatsunami and Masato Taki. Raftmlp: How much can be done without attention and with less spatial\\nlocality? arXiv preprint arXiv:2108.04384, 2021.\\n[70] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner,\\nJessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture\\nfor vision. In NeurIPS, volume 34, 2021.\\n[71] Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave,\\nArmand Joulin, Gabriel Synnaeve, Jakob Verbeek, and Hervé Jégou. Resmlp: Feedforward networks for\\nimage classiﬁcation with data-efﬁcient training. arXiv preprint arXiv:2105.03404, 2021.\\n[72] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\\nJégou. Training data-efﬁcient image transformers & distillation through attention. In ICML, 2021.\\n[73] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going deeper\\nwith image transformers. In ICCV, pages 32–42, 2021.\\n[74] Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In\\nInternational conference on machine learning, pages 1747–1756. PMLR, 2016.\\n[75] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, volume 30, 2017.\\n[76] Francesco Visin, Marco Ciccone, Adriana Romero, Kyle Kastner, Kyunghyun Cho, Yoshua Bengio,\\nMatteo Matteucci, and Aaron Courville. Reseg: A recurrent neural network-based model for semantic\\nsegmentation. In CVPRW, pages 41–48, 2016.\\n[77] Francesco Visin, Kyle Kastner, Kyunghyun Cho, Matteo Matteucci, Aaron Courville, and Yoshua Ben-\\ngio. Renet: A recurrent neural network based alternative to convolutional networks. arXiv preprint\\narXiv:1505.00393, 2015.\\n[78] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by\\npenalizing local predictive power. In NeurIPS, volume 32, 2019.\\n[79] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.\\nIn ICCV, 2021.\\n[80] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models,\\n2019.\\n[81] Tan Yu, Xu Li, Yunfeng Cai, Mingming Sun, and Ping Li. S 2-mlpv2: Improved spatial-shift mlp\\narchitecture for vision. arXiv preprint arXiv:2108.01072, 2021.\\n[82] Tan Yu, Xu Li, Yunfeng Cai, Mingming Sun, and Ping Li. S2-mlp: Spatial-shift mlp architecture for vision.\\nIn WACV, pages 297–306, 2022.\\n[83] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng\\nYan. Metaformer is actually what you need for vision. In CVPR, 2022.\\n[84] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng,\\nand Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In ICCV,\\npages 558–567, 2021.\\n13'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 13, 'page_label': '14'}, page_content='[85] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix:\\nRegularization strategy to train strong classiﬁers with localizable features. In ICCV, pages 6023–6032,\\n2019.\\n[86] David Junhao Zhang, Kunchang Li, Yunpeng Chen, Yali Wang, Shashwat Chandra, Yu Qiao, Luoqi Liu,\\nand Mike Zheng Shou. Morphmlp: A self-attention free, mlp-like backbone for image and video. arXiv\\npreprint arXiv:2111.12527, 2021.\\n[87] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\\nminimization. In ICLR, 2018.\\n[88] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation.\\nIn AAAI, volume 34, pages 13001–13008, 2020.\\n[89] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\\nthrough ade20k dataset. In CVPR, pages 633–641, 2017.\\n[90] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and Jiashi\\nFeng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021.\\n[91] Daquan Zhou, Yujun Shi, Bingyi Kang, Weihao Yu, Zihang Jiang, Yuan Li, Xiaojie Jin, Qibin Hou, and\\nJiashi Feng. Reﬁner: Reﬁning self-attention for vision transformers. arXiv preprint arXiv:2106.03714,\\n2021.\\n14'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 14, 'page_label': '15'}, page_content='A Societal impact\\nThe impact of this study on society has both positive and negative aspects. Here we discuss each.\\nOn the positive side, our proposal would promote modeling methods using LSTMs in computer vision.\\nThis study takes image patches as tokens and models their relationships with LSTMs. Although\\nLSTMs have been used in computer vision, designing image recognition with a module that includes\\nLSTMs in the spatial direction as the main elements, as our study does, is new. It is exciting to see if\\nthis design beneﬁts computer vision tasks other than image classiﬁcation. Thus, our study would be\\nan impetus for further research on its application to various computer vision tasks.\\nOn the other side, our architecture may increase the carbon dioxide footprint: the study of new\\narchitectures for vision, such as Sequencer, requires iterative training of models for long periods\\nto optimize the model’s design. In particular, Sequencer is not a FLOPs-friendly design, and the\\namount of carbon dioxide emitted during training is likely to be high. Therefore, considering the\\nenvironmental burden caused by the training of Sequencers, research to reduce the computational\\ncost of Sequencers is also desired by society.\\nB Implementation details\\nIn this section, implementation details are supplemented. We describe the pseudocode of the\\nBiLSTM2D layer, the architecture details, settings for training on IN-1K, and introduce settings for\\ntransfer learning.\\nB.1 Pseudocode\\nAlgorithm 1 Pseudocode of BiLSTM2D layer.\\n# B: batch size H: height, W: width, C: channel, D: hidden dimension\\n# x: input tensor of shape (B, H, W, C)\\n### initialization ###\\nself.rnn_v = nn.LSTM(C, D, num_layers=1, batch_first=True, bias=True, bidirectional=True)\\nself.rnn_h = nn.LSTM(C, D, num_layers=1, batch_first=True, bias=True, bidirectional=True)\\nself.fc = nn.Linear(4 * D, C)\\n### forward ###\\ndef forward(self, x):\\nv, _ = self.rnn_v(x.permute(0, 2, 1, 3).reshape(-1, H, C))\\nv = v.reshape(B, W, H, -1).permute(0, 2, 1, 3)\\nh, _ = self.rnn_h(x.reshape(-1, W, C))\\nh = h.reshape(B, H, W, -1)\\nx = torch.cat([v, h], dim=-1)\\nx = self.fc(x)\\nreturn x\\nB.2 Architecture details\\nThis subsection describes Sequencer’s architecture. The architectural details are shown in Table 4\\nand 5.\\nSequencer2D-S is based on a ViP-S/7-like architecture. We intend to directly compare the BiLSTM2D\\nlayer in Sequencer2D, which has a similar structure, with the Permute-MLP layer in ViP-S/7. Table 4\\nis a summary of the architecture. In keeping with ViP, the ﬁrst stage of Sequencers involves patch\\nembedding with a 7x7 kernel. The second stage of Sequencers performs patch embedding with\\na 2x2 kernel, but the following two stages have no downsampling. The classiﬁer of Sequencers\\nthen continues with layer normalization (LN) [1], followed by global average pooling and a linear\\nlayer. The number of blocks of Sequencer2D-S, Sequencer2D-M, and Sequencer2D-L correspond to\\nViP-S/7, ViP-M/7, and ViP-L/7, respectively. However, as described in Appendix C, we conﬁgure\\nthe dimension of the block to be different from ViP-M/7 and ViP-L/7 for Sequencer2D-M and\\nSequencer2D-L, respectively, because high dimension causes over-ﬁtting.\\nVSequencer is a bit different from Sequencer2D in that it is non-hierarchical architecture. Table 5\\ndeﬁne that no downsampling is performed in the second stage, instead of downsampling with 14x14\\nkernel for patch embedding in the ﬁrst stage. In addition, we match the dimension of the blocks in\\nthe ﬁrst stage to the dimension of the subsequent blocks.\\n15'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 15, 'page_label': '16'}, page_content='Following the overall architecture, we describe the details of the modules not mentioned in the main\\ntext. Sequencer2D block and the Vanilla Sequencer block use LNs [1] for the normalization layers.\\nWe follow previous studies for the channel MLPs of these blocks and employ MLPs with Gaussian\\nError Linear Units (GELUs) [ 25] as the activation function; the ratio of increasing dimension in\\nMLPs is uniformly 3x, as shown in Table 4 and 5.\\nB.3 IN-1K settings\\nOn IN-1K dataset [39], we utilize the hyper-parameters displayed in Table 6 to scratch train models\\nin subsections 4.1 and 4.3. All Sequencer variants, including the models in the ablation study, follow\\nalmost the same settings for pre-training. However, the stochastic depth rate and batch size are\\nadjusted depending on the model variant. The models in the ablation study are Sequencer2D-S based\\nbecause of the following Sequencer2D-S settings.\\nThe ﬁne-tuning Sequencer2D-L↑3922 in subsection 4.2 has slightly different hyper-parameters than\\nthe pre-training models. There are changes in the settings for the number of epochs and learning rate\\nbecause it uses trained weights, so there is no need to increase these hyper-parameters. In addition,\\nwe used crop ratio 0.875 during testing in the pre-training models instead of crop ratio 1.0 in the\\nﬁne-tuning model.\\nB.4 Transfer learning settings\\nDetails of the datasets used for transfer learning in subsection 4.4 are shown in Table 7. This summary\\nincludes for each dataset CIFAR-10 [38], CIFAR-100 [38], Flowers-102 [55], and Stanford Cars [37],\\nthe number of training images, test images, and number of categories are listed.\\nTable 6 demonstrates the hyperparameters used in transfer learning with these datasets. The training\\nepochs are especially adjusting to the datasets and changing them. The reason for this is attributable\\nto the different sizes of the datasets.\\nC More results\\nThis section discusses additional results that could not be addressed in the main text. The contents of\\nthe experiment consist of three parts: an evaluation of robustness in subsection C.1, an evaluation of\\ngeneralization performance in subsection C.2, and a discussion of over-ﬁtting in subsection C.3.\\nC.1 Robustness\\nIn this subsection, we evaluate the robustness of Sequencer. There are two main evaluation methods,\\nbenchmark datasets and adversarial attacks.\\nEvaluation with benchmark datasets reveals nice robustness of Sequencer. The evaluation results\\nare summarized in Table 8. We test our models, trained on only IN-1K, on several datasets such as\\nImageNet-A/R/Sketch/C (IN-A/R/Sketch/C) [26, 23, 78, 24] to evaluate robustness. We evaluate\\nour models on IN-C with mean corruption error (mCE), and on other datasets with top-1 accuracy.\\nThis result leads us to suggest that for models with a similar number of parameters, Sequencer is\\nconquered by Swin and is robust enough to be competitive with ConvNeXt. Table 9 shows detail\\nevaluation on IN-C. According to the results, it is understood that Sequencer is more immune to\\ncorruptions other than Noise than Swin and ConvNeXt, and, in particular, the model is less sensitive\\nto weather conditions.\\nSequencers are tolerant of principal adversarial attacks. We evaluate robustness using the single-step\\nattack algorithm FGSM [ 18] and multi-step attack algorithm PGD [ 52]. Both algorithms give a\\nperturbation of max magnitude 1. For PGD, we choose steps 5 and step size 0.5. This setup is based\\non RVT [53]. Table 9 indicates that Sequencer2D-L defeats in both FGSM and PGD compared to\\nother models. Thus, Sequencer has an advantage over conventional models, such as RVT, which tout\\nrobustness on these adversarial attacks.\\n16'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 16, 'page_label': '17'}, page_content='Table 4: Variants of Sequencer2D and details . \"d\" denotes the input/output dimension, and D\\ndenotes the hidden dimension as above. \"↓n\" (e.g., ↓2) shows the stride of the downsampling is n\\nSequencer2D-S Sequencer2D-M Sequencer2D-L\\nstage 1\\nPatch Embedding↓7 Patch Embedding↓7 Patch Embedding↓7\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM2D: 192d\\nD= 48\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×4\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM2D: 192d\\nD= 48\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×4\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM2D: 192d\\nD= 48\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×8\\nstage 2\\nPatch Embedding↓2 Patch Embedding↓2 Patch Embedding↓2\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM2D: 384d\\nD= 96\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×3\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM2D: 384d\\nD= 96\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×3\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM2D: 384d\\nD= 96\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×8\\nstage 3\\nPoint-wise Linear Point-wise Linear Point-wise Linear\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM2D: 384d\\nD= 96\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×8\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM2D: 384d\\nD= 96\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×14\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM2D: 384d\\nD= 96\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×16\\nstage 4\\nPoint-wise Linear Point-wise Linear Point-wise Linear\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM2D: 384d\\nD= 96\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×3\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM2D: 384d\\nD= 96\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×3\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM2D: 384d\\nD= 96\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×4\\nclassiﬁer Layer Norm., Global Average Pooling, Linear\\nC.2 Generalization ability\\nThe generalization ability of Sequencers is also impressive. We evaluate our models on ImageNet-\\nReal/V2 (IN-Real/V2) [2, 62] to test their generalization performance: IN-Real is a re-labeled dataset\\nof the IN-1K validation set, and IN-V2 is the dataset that re-collects the IN-1K validation set. Table 8\\nshows the results of evaluating the top-1 accuracy on both datasets. We reveal an understanding of\\nthe Sequencer’s excellent generalization ability.\\nC.3 Over-ﬁtting\\nWide Sequencers tend to be over-trained. We scratch-train Sequencer2D-Lx1.3, which has 4/3 times\\nthe dimension of each layer of Sequencer2D-L, on IN-1K. The training utilizes the same conditions\\nas Sequencer2D-L. Consequently, as Table 10 shows, Sequencer2D-Lx1.3 has 0.8% less accuracy\\nthan Sequencer2D-L. Figure 6 illustrates the cross-entropy evolution and top-1 accuracy on IN-1K\\nvalidation set for the two models. On the one hand, cross-entropy decreased on Sequencer2D-L in\\nthe last 100 epochs. On the other hand, Sequencer2D-Lx1.3 is increasing. Thus, widening Sequencer\\nis counterproductive for training.\\nC.4 Semantic segmentation\\nWe evaluate models with Sequencer as the backbone for a semantic segmentation task. We trained and\\nevaluated on ADE20K dataset [89], a well-known scene parsing benchmark. The dataset consists of\\nthe training set with about 20k images and the validation set with about 2k, covering 150 ﬁne-grained\\nsemantic classes. We employed Sequencer as the backbone of SemanticFPN [36] to train and evaluate\\nsemantic segmentation. The training adopts a batch size of 32 and AdamW [ 50] with the initial\\nlearning rate of 2e-4, decay in the polynomial decay schedule with a power of 0.9, and 40k iterations\\n17'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 17, 'page_label': '18'}, page_content='Table 5: Variants of VSequencer and details . \"d\" denotes the input/output dimension, and D\\ndenotes the hidden dimension as above. \"↓n\" (e.g., ↓2) shows the stride of the downsampling is n\\nVSequencer-S VSequencer-S(H) VSequencer-S(PE)\\nstage 1\\nPatch Embedding↓14 Patch Embedding↓7 Patch Embedding↓14\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM: 384d\\nD= 192\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×4\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM: 192d\\nD= 96\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×4\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM: 384d\\nD= 192\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×4\\nstage 2\\nPoint-wise Linear Patch Embedding↓2 Point-wise Linear\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM: 384d\\nD= 192\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×3\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM: 384d\\nD= 192\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×3\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM: 384d\\nD= 192\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×3\\nstage 3\\nPoint-wise Linear Point-wise Linear Point-wise Linear\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM: 384d\\nD= 192\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×8\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM: 384d\\nD= 192\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×8\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM: 384d\\nD= 192\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×8\\nstage 4\\nPoint-wise Linear Point-wise Linear Point-wise Linear\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM: 384d\\nD= 192\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×3\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM: 384d\\nD= 192\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×3\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nBiLSTM: 384d\\nD= 192\\nMLP: 3 exp. ratio\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb ×3\\nclassiﬁer Layer Norm., Global Average Pooling, Linear\\n150 200 250 3000.75\\n0.8\\n0.85\\n0.9\\n0.95\\n1\\nSequencer2D-L\\nSequencer2D-Lx1.3\\nEpochs\\nCross Entropy\\n(a) Cross entropy\\n150 200 250 30078\\n79\\n80\\n81\\n82\\n83\\n84\\nSequencer2D-L\\nSequencer2D-Lx1.3\\nEpochs\\nImageNet- 1k Top- 1 A cc.( % ) (b) Top-1 accuracy\\nFigure 6: Comparison of different model widths . (a) is cross entropy, (b) is top-1 accuracy\\ncomparison, on IN-1K validation set. The blue curve represents the original Sequencer2D-L, which\\ndid not produce any problems and is learning all the way through. In contrast, the green curve\\nrepresents the wider Sequencer2D-Lx1.3. This model stalls in the second half and is somewhat\\ndegenerate.\\nof training. These settings follow Metaformer [83]. Table 3 of the result indicates that Sequencer has\\nthe generalization for segmentation is comparable to other leading models.\\nC.5 Object Detection\\nWe evaluate Sequencer on COCO benchmark [45]. The dataset consists of 118k training images and\\n5k validation images. Sequencer with ImageNet pre-trained weights is employed as the backbone of\\nRetinaNet [44]. Following [ 44], we employ AdamW, batch size of 16, and 1×training schedule.\\nTable 11 shows that Sequencer is not suited for existing standard object detection models such as\\n18'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 18, 'page_label': '19'}, page_content='Table 6: Hyper-parameters. ↑denotes ﬁne-tuning pre-trained model on IN-1K. Multiple values are\\nfor each model, respectively.\\nTraining conﬁg. Sequencer2D-S/M/L Sequencer2D-L↑ Sequencer2D-S↑/M↑/L↑\\n2242 3922 2242\\ndataset IN-1K [39] IN-1K [39] CIFAR10, 100, Flowers, Cars\\noptimizer AdamW [50] AdamW [50] AdamW [50]\\nbase learning rate 2e-3/1.5e-3/1e-3 5e-5 1e-4\\nweight decay 0.05 1e-8 1e-4\\noptimizer ϵ 1e-8 1e-8 1e-8\\noptimizer momentum β1,= 0.9,β2=0.999 β1,= 0.9,β2=0.999 β1,= 0.9,β2=0.999\\nbatch size 2048/1536/1024 512 512\\ntraining epochs 300 30 CIFAR: 200, Others: 1000\\nlearning rate schedule cosine decay cosine decay cosine decay\\nlower learning rate bound 1e-6 1e-6 1e-6\\nwarmup epochs 20 None 5\\nwarmup schedule linear None linear\\nwarmup learning rate 1e-6 None 1e-6\\ncooldown epochs 10 None 10\\ncrop ratio 0.875 1.0 0.875\\nrandaugment [11] (9, 0.5) (9, 0.5) (9, 0.5)\\nmixup α[87] 0.8 0.8 0.8\\ncutmix α[85] 1.0 1.0 1.0\\nrandom erasing [88] 0.25 0.25 None\\nlabel smoothing [66] 0.1 0.1 0.1\\nstochastic depth [30] 0.1/0.2/0.4 0.4 0.1/0.2/0.4\\ngradient clip None None 1\\nTable 7: Transfer learning datasets.\\nDataset Train Size Test size #Classes\\nCIFAR-10 [38] 50,000 10,000 10\\nCIFAR-100 [38] 50,000 10,000 100\\nFlowers-102 [55] 2,040 6,149 102\\nStanford Cars [37] 8,144 8,041 196\\nRetinaNet. It shows no improvement trend for model scaling. It also struggles to detect small objects,\\nmaking RNN-based object detection models an issue to consider in the future.\\nC.6 More studies\\nMethod of merge As shown in Figure 2, \"concatenate\" is used to merge the vertical BiLSTM\\nand horizontal BiLSTM outputs but \" add\" can also be used. See Table 12a for the result of the\\nexperiment.\\nD Effective receptive ﬁeld\\nThis section covers in detail the effective receptive ﬁelds (ERFs) [ 51] used in the visualization in\\nsubsection 4.5. First, we explain how the visualized effective receptive ﬁelds are obtained. Second,\\nwe present other visualization results not addressed in the main text. The ERF’s calculations in this\\npaper are based on [14].\\nD.1 Calculation of visualized ERFs\\nThe ERF [51] is a technique for calculating the pixels that contribute to the center of a output feature\\nmaps of a neural network. Let I ∈Rn×h×w×c be a input image collection and O ∈Rn×h′×w′×c′\\nbe the output feature map collection. The center of the output feature map can be expressed as\\nO:,⌊h′/2⌋,⌊w′/2⌋,:, where ⌊·⌋is the ﬂoor function. Each element of the derivative of Oi,⌊h′/2⌋,⌊w′/2⌋,j\\nto I, i.e.,\\n∂(\\n∑\\ni,j Oi,⌊h′/2,⌋⌊w′/2⌋,j)\\n∂I , represents to what extent the center of the output feature map\\n19'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 19, 'page_label': '20'}, page_content='Table 8: The robustness is evaluated on IN-A [26] (top-1 accuracy), IN-R [23] (top-1 accuracy),\\nIN-Sketch [78] (top-1 accuracy), IN-C [ 24] (mCE), FGSM [ 18] (top-1 accuracy), and PGD [ 52]\\n(top-1 accuracy). The generalization ability is evaluated on IN-Real [2] and IN-V2 [62]. We denote\\nthe higher as better value as ↑and the lower as better value as ↓. Rather than those reported in the\\noriginal paper, the values we observed are marked with †. If the model name has †, it means that we\\nobserved all the metrics of the model.\\nModel #Param. FLOPs Clean(↑) A(↑) R(↑) Sk.(↑) C(↓) FGSM(↑) PGD(↑) Real(↑) V2(↑)\\nSwin-T [48] 28M 4.5G 81.2 21.6 41.3 29.1 62.0 33.7 7.3 86.7† 69.6†\\nConvNeXt-T [49] 29M 4.5G 82.1 24.2 47.2 33.8 53.2 37.8† 10.5† 87.3† 71.0†\\nRVT-S* [53] 23M 4.7G 81.9 25.7 47.7 34.7 49.4 51.8 28.2 - -\\nSequencer2D-S 28M 8.4G 82.3 26.7 45.1 33.4 53.0 49.2 25.0 87.4 71.8\\nSequencer2D-M 38M 11.1G 82.8 30.5 46.3 34.7 51.8 50.8 26.3 87.6 72.5\\nSwin-S [48]† 50M 8.7G 83.2 32.5 45.2 32.3 54.9 45.9 18.1 87.7 72.1\\nConvNeXt-S† [49] 50M 8.7G 83.1 31.3 49.6 37.1 49.5 46.1 17.7 88.1 72.5\\nSequencer2D-L 54M 16.6G 83.4 35.5 48.1 35.8 48.9 53.1 30.9 87.9 73.4\\nSwin-B [48] 88M 15.4G 83.4 35.8 46.6 32.4 54.4 49.2 21.3 89.2† 75.6†\\nConvNeXt-B [49] 89M 15.4G 83.8 36.7 51.3 38.2 46.8 47.5† 18.3† 88.4† 73.7†\\nRVT-B* [53] 92M 17.7G 82.6 28.5 48.7 36.0 46.8 53.0 29.9 - -\\nTable 9: Details of robustness evaluation with IN-C.\\nNoise Blur Weather Digital\\nModel mCE Gauss. Shot Impulse Defocus Glass Motion Zoom Snow Frost Fog Bright Contrast Elastic Pixel JPEG\\nSwin-S [48] 54.9 42.9 44.9 43.2 61.3 74.1 56.6 67.5 50.8 48.5 46.0 44.1 42.1 68.9 62.1 70.7\\nConvNeXt-S [49] 49.5 38.1 39.1 37.9 57.8 72.5 51.8 61.9 46.1 43.8 44.6 39.6 37.6 66.7 55.1 50.1\\nSequencer2D-L 48.9 43.3 42.0 41.4 55.2 71.0 51.8 63.3 44.2 41.0 41.9 37.1 33.8 66.6 50.4 51.1\\nchanges for each perturbation of each pixel in each input image. Adding these together for all\\nimages and channels, we can calculate the average pixel contribution for all input images, which\\ncan be activated with a Rectiﬁed Linear Unit (ReLU) to get the positively contributing pixel values\\nP ∈Rn×h×w×c, deﬁned by\\nP = ReLU\\n\\uf8eb\\n\\uf8ed\\n∂\\n(∑\\ni,j Oi,⌊h′/2,⌋⌊w′/2⌋,j\\n)\\n∂I\\n\\uf8f6\\n\\uf8f8. (8)\\nFurthermore, the score S ∈Rh×w is calculated by\\nS = log10\\n\\uf8eb\\n\\uf8ed∑\\ni,j\\nPi,:,:,j + 1\\n\\uf8f6\\n\\uf8f8, (9)\\nand S is called the effective receptive ﬁeld.\\nNext, deﬁne a visualized effective receptive ﬁeld based on the effective receptive ﬁeld. We want to\\ncompare the effective receptive ﬁelds across models. We, therefore, calculate the score Smodel for\\neach model and rescale Smodel from 0 to 1 across the models. The tensor calculated in this way is\\ncalled the visualized effective receptive ﬁeld.\\nThe derivatives used in these deﬁnitions are efﬁcient if they take advantage of the auto-grad mecha-\\nnism. Indeed, we also relied on the automatic auto-grad function on PyTorch [56] to calculate the\\neffective receptive ﬁelds.\\nTable 10: Comparison of accuracy for different model widths.\\nModel #Params. FLOPs Acc.\\nSequencer2D-L 54M 16.6G 83.4\\nSequencer2D-Lx1.3 96M 29.4G 83.0\\n20'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 20, 'page_label': '21'}, page_content='Table 11: Object detection results on COCO dataset [45]\\nBackbone Params (M) AP AP50 AP75 APS APM APL\\nResNet-18 [22] 21.3 31.8 49.6 33.6 16.3 34.3 43.2\\nPoolFormer-S12 [83] 21.7 36.2 56.2 38.2 20.8 39.1 48.0\\nSequencer2D-S 37.3 33.6 54.8 34.8 15.3 37.5 50.2\\nResNet-50 [22] 37.7 36.3 55.3 38.6 19.3 40.0 48.8\\nPoolFormer-S24 [83] 31.1 38.9 59.7 41.3 23.3 42.1 51.8\\nSequencer2D-M 47.9 34.5 55.5 35.9 15.0 39.0 51.6\\nResNet-101 [22] 56.7 38.5 57.8 41.2 21.4 42.6 51.1\\nPoolFormer-S36 [83] 40.6 39.5 60.5 41.8 22.5 42.9 52.4\\nSequencer2D-L 63.9 35.0 56.4 36.5 16.5 39.6 51.6\\nTable 12: More Sequencer ablation experiments.\\n(a) Method of merge\\nUnion #Params. FLOPs Acc.\\nadd 27M 8.0G 82.2\\nconcatnate 28M 8.4G 82.3\\nD.2 More visualization of ERFs\\nWe introduce additional visualization and concrete visualization method. We experiment with\\nvisualization using input images of two different resolutions.\\nWe visualize the effective receptive ﬁelds of Sequencer2D-S and comparative models by using 2242\\nresolution images. The method is applied to the following models for comparing: ResNet-50 [22],\\nConvNeXt-T [49], CycleMLP-B2 [7], DeiT-S [72], Swin-T [48], GFNet-S [61], and ViP-S/7 [28].\\nThe object to be visualized is the output for each block, and the effective receptive ﬁelds are calculated.\\nFor example, in the case of Sequencer2D-S, the effective receptive ﬁelds are calculated for the output\\nof each Sequencer block. We are rescaling within a value between 0 and 1 for the whole to effective\\nreceptive ﬁelds for each model block.\\nThe effective receptive ﬁelds of Sequencer2D-S and comparative models are then visualized using\\ninput images with a resolution of 448 2. The reason for running experiments is to verify how the\\nreceptive ﬁeld is affected when the input resolution is increased compared to the 224 2 resolution\\ninput image. Sequencer2D-S compare with ResNet-50 [22], ConvNeXt-T [49], CycleMLP-B2 [7],\\nDeiT-S [72], and GFNet-S [28]. The method of visualization of the effective receptive ﬁeld follows\\nthe case of input images with a resolution of 2242.\\nSequencer has very distinctive cruciform ERFs in all layers. Table 7, 8, 9, 10, 11, 12, 13, and 14\\nillustrates this fact for 224 2 resolution input images. Furthermore, as shown in Table 15, 16, 17,\\n18 and 19, we observe the same trend when the double resolution. The ERFs are structurally quite\\ndifferent from the ERFs other than ViP, which have a similar structure. ViP’s ERFs have, on average,\\nsome also coverage except for the cruciforms. In contrast, Sequencer’s ERFs are limited to the\\ncruciform and its neighborhood.\\nIt is interesting to note that Sequencer, with its characteristic ERFs, achieves high accuracy. It will\\nbe helpful for future architecture development because of the possibility of creating Sequencer-like\\nERFs outside of LSTM.\\n21'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 21, 'page_label': '22'}, page_content='(a) Block 1\\n (b) Block 2\\n (c) Block 3\\n (d) Block 4\\n (e) Block 5\\n (f) Block 6\\n (g) Block 7\\n(h) Block 8\\n (i) Block 9\\n (j) Block 10\\n (k) Block 11\\n (l) Block 12\\n (m) Block 13\\n (n) Block 14\\n(o) Block 15\\n (p) Block 16\\n (q) Block 17\\n (r) Block 18\\nFigure 7: ERFs in Sequencer2D-S on images with resolution 2242.\\n(a) Block 1\\n (b) Block 2\\n (c) Block 3\\n (d) Block 4\\n (e) Block 5\\n (f) Block 6\\n (g) Block 7\\n(h) Block 8\\n (i) Block 9\\n (j) Block 10\\n (k) Block 11\\n (l) Block 12\\n (m) Block 13\\n (n) Block 14\\n(o) Block 15\\n (p) Block 16\\nFigure 8: ERFs in ResNet-50 [22] on images with resolution 2242.\\n22'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 22, 'page_label': '23'}, page_content='(a) Block 1\\n (b) Block 2\\n (c) Block 3\\n (d) Block 4\\n (e) Block 5\\n (f) Block 6\\n (g) Block 7\\n(h) Block 8\\n (i) Block 9\\n (j) Block 10\\n (k) Block 11\\n (l) Block 12\\n (m) Block 13\\n (n) Block 14\\n(o) Block 15\\n (p) Block 16\\n (q) Block 17\\n (r) Block 18\\nFigure 9: ERFs in ConvNeXt-T [49] on images with resolution 2242.\\n(a) Block 1\\n (b) Block 2\\n (c) Block 3\\n (d) Block 4\\n (e) Block 5\\n (f) Block 6\\n (g) Block 7\\n(h) Block 8\\n (i) Block 9\\n (j) Block 10\\n (k) Block 11\\n (l) Block 12\\n (m) Block 13\\n (n) Block 14\\n(o) Block 15\\n (p) Block 16\\n (q) Block 17\\n (r) Block 18\\nFigure 10: ERFs in CycleMLP-B2 [7] on images with resolution 2242.\\n(a) Block 1\\n (b) Block 2\\n (c) Block 3\\n (d) Block 4\\n (e) Block 5\\n (f) Block 6\\n (g) Block 7\\n(h) Block 8\\n (i) Block 9\\n (j) Block 10\\n (k) Block 11\\n (l) Block 12\\nFigure 11: ERFs in DeiT-S [72] on images with resolution 2242.\\n23'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 23, 'page_label': '24'}, page_content='(a) Block 1\\n (b) Block 2\\n (c) Block 3\\n (d) Block 4\\n (e) Block 5\\n (f) Block 6\\n (g) Block 7\\n(h) Block 8\\n (i) Block 9\\n (j) Block 10\\n (k) Block 11\\n (l) Block 12\\nFigure 12: ERFs in Swin-T [48] on images with resolution 2242.\\n(a) Block 1\\n (b) Block 2\\n (c) Block 3\\n (d) Block 4\\n (e) Block 5\\n (f) Block 6\\n (g) Block 7\\n(h) Block 8\\n (i) Block 9\\n (j) Block 10\\n (k) Block 11\\n (l) Block 12\\n (m) Block 13\\n (n) Block 14\\n(o) Block 15\\n (p) Block 16\\n (q) Block 17\\n (r) Block 18\\n (s) Block 19\\nFigure 13: ERFs in GFNet-S [61] on images with resolution 2242.\\n(a) Block 1\\n (b) Block 2\\n (c) Block 3\\n (d) Block 4\\n (e) Block 5\\n (f) Block 6\\n (g) Block 7\\n(h) Block 8\\n (i) Block 9\\n (j) Block 10\\n (k) Block 11\\n (l) Block 12\\n (m) Block 13\\n (n) Block 14\\n(o) Block 15\\n (p) Block 16\\n (q) Block 17\\n (r) Block 18\\nFigure 14: ERFs in ViP-S/7 [28] on images with resolution 2242.\\n24'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 24, 'page_label': '25'}, page_content='(a) Block 1\\n (b) Block 2\\n (c) Block 3\\n (d) Block 4\\n (e) Block 5\\n (f) Block 6\\n (g) Block 7\\n(h) Block 8\\n (i) Block 9\\n (j) Block 10\\n (k) Block 11\\n (l) Block 12\\n (m) Block 13\\n (n) Block 14\\n(o) Block 15\\n (p) Block 16\\n (q) Block 17\\n (r) Block 18\\nFigure 15: ERFs in Sequencer2D-S on images with resolution 4482.\\n(a) Block 1\\n (b) Block 2\\n (c) Block 3\\n (d) Block 4\\n (e) Block 5\\n (f) Block 6\\n (g) Block 7\\n(h) Block 8\\n (i) Block 9\\n (j) Block 10\\n (k) Block 11\\n (l) Block 12\\n (m) Block 13\\n (n) Block 14\\n(o) Block 15\\n (p) Block 16\\nFigure 16: ERFs in ResNet-50 [22] on images with resolution 4482.\\n25'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 25, 'page_label': '26'}, page_content='(a) Block 1\\n (b) Block 2\\n (c) Block 3\\n (d) Block 4\\n (e) Block 5\\n (f) Block 6\\n (g) Block 7\\n(h) Block 8\\n (i) Block 9\\n (j) Block 10\\n (k) Block 11\\n (l) Block 12\\n (m) Block 13\\n (n) Block 14\\n(o) Block 15\\n (p) Block 16\\n (q) Block 17\\n (r) Block 18\\nFigure 17: ERFs in ConvNeXt-T [49] on images with resolution 4482.\\n(a) Block 1\\n (b) Block 2\\n (c) Block 3\\n (d) Block 4\\n (e) Block 5\\n (f) Block 6\\n (g) Block 7\\n(h) Block 8\\n (i) Block 9\\n (j) Block 10\\n (k) Block 11\\n (l) Block 12\\n (m) Block 13\\n (n) Block 14\\n(o) Block 15\\n (p) Block 16\\n (q) Block 17\\n (r) Block 18\\nFigure 18: ERFs in CycleMLP-B2 [7] on images with resolution 4482.\\n(a) Block 1\\n (b) Block 2\\n (c) Block 3\\n (d) Block 4\\n (e) Block 5\\n (f) Block 6\\n (g) Block 7\\n(h) Block 8\\n (i) Block 9\\n (j) Block 10\\n (k) Block 11\\n (l) Block 12\\nFigure 19: ERFs in DeiT-S [72] on images with resolution 4482.\\n26')]\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents(\"lstm.pdf\")\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading documents...\n",
      "chunking text...\n",
      "page_content='computer vision. Here we propose Sequencer, a novel and competitive architecture\n",
      "alternative to ViT that provides a new perspective on these issues. Unlike ViTs,\n",
      "Sequencer models long-range dependencies using LSTMs rather than self-attention\n",
      "layers. We also propose a two-dimensional version of Sequencer module, where an\n",
      "LSTM is decomposed into vertical and horizontal LSTMs to enhance performance.\n",
      "Despite its simplicity, several experiments demonstrate that Sequencer performs\n",
      "impressively well: Sequencer2D-L, with 54M parameters, realizes 84.6% top-1\n",
      "accuracy on only ImageNet-1K. Not only that, we show that it has good transfer-\n",
      "ability and the robust resolution adaptability on double resolution-band. Our source\n",
      "code is available at https://github.com/okojoalg/sequencer.\n",
      "1 Introduction\n",
      "Sequencer2D-S\n",
      "Sequencer2D-M\n",
      "Sequencer2D-L\n",
      "0 20 40 60 80 100\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "ConvNeXt\n",
      "Swin\n",
      "ViP\n",
      "CycleMLP\n",
      "GFNet\n",
      "Sequencer(ours)\n",
      "Number of Parameters(M)\n",
      "ImageNet-1k Top-1 Acc.(%)' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-01-13T02:09:12+00:00', 'author': '', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'lstm.pdf', 'total_pages': 26, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents(\"lstm.pdf\")\n",
    "chunks = chunk_text(documents)\n",
    "print(chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "def create_vector_db(chunks):\n",
    "    \"\"\"\n",
    "    创建本地向量数据库并保存文本块\n",
    "    \n",
    "    Args:\n",
    "        chunks: 文本块列表\n",
    "        persist_directory: 向量数据库保存的目录\n",
    "        \n",
    "    Returns:\n",
    "        创建好的向量数据库\n",
    "    \"\"\"\n",
    " \n",
    "    \n",
    "    # 创建Chroma向量数据库\n",
    "    chromadb = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        collection_name=\"navie_rag\",\n",
    "        embedding=HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en\"),\n",
    "        persist_directory=\"./rag_chroma\"\n",
    "    )\n",
    "    \n",
    "    # 将Chroma向量数据库持久化到磁盘\n",
    "   \n",
    "    print(f\"Chroma向量数据库已创建并保存到rag_chroma\")\n",
    "  \n",
    "    return chromadb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136a81e1196b4e329b25436b01ed411a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c285144b6f3f48348a6f0a2574690bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b3de44640841d68cfdf04dc95a2e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/90.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a24c821d0074ff59170064c346819aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a24e4d3a48047129e422851752c52f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/719 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6328a6b6a2294cee82637a7575454a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42e28f231c14904a96515a88b3a4b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7b6be5c9594cf888ba4963bdb75dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3501b74b939647dba6ed20ccf0987aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f2c57453474f29854f8a1f5f350af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca6996379af4151a9d539ff863fac2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma向量数据库已创建并保存到rag_chroma\n"
     ]
    }
   ],
   "source": [
    "db = create_vector_db(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_related_context(query, db, k=1):\n",
    "    \"\"\"\n",
    "    根据用户查询检索相关上下文\n",
    "    \n",
    "    Args:\n",
    "        query: 用户查询\n",
    "        db: 向量数据库\n",
    "        k: 返回的相关文档数量\n",
    "        \n",
    "    Returns:\n",
    "        检索到的相关上下文\n",
    "    \"\"\"\n",
    "    # 从向量数据库中检索与查询相关的文档\n",
    "    retriever = db.as_retriever(search_type=\"similarity\",search_kwargs={\"k\": k})\n",
    "    print(\"Retrieving\")\n",
    "    related_chunks = retriever.invoke(query)\n",
    "    \n",
    "\n",
    "    return related_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain LSTM model in one line\"\n",
    "\n",
    "relevant_chunks = retrieve_related_context(query, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk-0\n",
      "page_content='# x: input tensor of shape (B, H, W, C)\n",
      "### initialization ###\n",
      "self.rnn_v = nn.LSTM(C, D, num_layers=1, batch_first=True, bias=True, bidirectional=True)\n",
      "self.rnn_h = nn.LSTM(C, D, num_layers=1, batch_first=True, bias=True, bidirectional=True)\n",
      "self.fc = nn.Linear(4 * D, C)\n",
      "### forward ###\n",
      "def forward(self, x):\n",
      "v, _ = self.rnn_v(x.permute(0, 2, 1, 3).reshape(-1, H, C))\n",
      "v = v.reshape(B, W, H, -1).permute(0, 2, 1, 3)\n",
      "h, _ = self.rnn_h(x.reshape(-1, W, C))\n",
      "h = h.reshape(B, H, W, -1)\n",
      "x = torch.cat([v, h], dim=-1)\n",
      "x = self.fc(x)\n",
      "return x\n",
      "B.2 Architecture details\n",
      "This subsection describes Sequencer’s architecture. The architectural details are shown in Table 4\n",
      "and 5.\n",
      "Sequencer2D-S is based on a ViP-S/7-like architecture. We intend to directly compare the BiLSTM2D\n",
      "layer in Sequencer2D, which has a similar structure, with the Permute-MLP layer in ViP-S/7. Table 4\n",
      "is a summary of the architecture. In keeping with ViP, the ﬁrst stage of Sequencers involves patch' metadata={'author': '', 'creationdate': '2023-01-13T02:09:12+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-01-13T02:09:12+00:00', 'page': 14, 'page_label': '15', 'producer': 'pdfTeX-1.40.21', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': 'lstm.pdf', 'subject': '', 'title': '', 'total_pages': 26, 'trapped': '/False'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(relevant_chunks):\n",
    "  print(f\"Chunk-{i}\")\n",
    "  print(chunk)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(relevant_chunks: List[Document]) -> str:\n",
    "    \"\"\"\n",
    "    Builds a context string from retrieved relevant document chunks.\n",
    "\n",
    "    Parameters:\n",
    "    relevant_chunks (List[Document]): A list of retrieved relevant document chunks.\n",
    "\n",
    "    Returns:\n",
    "    str: A concatenated string containing the content of the relevant chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Context is built from relevant chunks\")\n",
    "    context = \"\\n\\n\".join([chunk.page_content for chunk in relevant_chunks])\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context is built from relevant chunks\n",
      "# x: input tensor of shape (B, H, W, C)\n",
      "### initialization ###\n",
      "self.rnn_v = nn.LSTM(C, D, num_layers=1, batch_first=True, bias=True, bidirectional=True)\n",
      "self.rnn_h = nn.LSTM(C, D, num_layers=1, batch_first=True, bias=True, bidirectional=True)\n",
      "self.fc = nn.Linear(4 * D, C)\n",
      "### forward ###\n",
      "def forward(self, x):\n",
      "v, _ = self.rnn_v(x.permute(0, 2, 1, 3).reshape(-1, H, C))\n",
      "v = v.reshape(B, W, H, -1).permute(0, 2, 1, 3)\n",
      "h, _ = self.rnn_h(x.reshape(-1, W, C))\n",
      "h = h.reshape(B, H, W, -1)\n",
      "x = torch.cat([v, h], dim=-1)\n",
      "x = self.fc(x)\n",
      "return x\n",
      "B.2 Architecture details\n",
      "This subsection describes Sequencer’s architecture. The architectural details are shown in Table 4\n",
      "and 5.\n",
      "Sequencer2D-S is based on a ViP-S/7-like architecture. We intend to directly compare the BiLSTM2D\n",
      "layer in Sequencer2D, which has a similar structure, with the Permute-MLP layer in ViP-S/7. Table 4\n",
      "is a summary of the architecture. In keeping with ViP, the ﬁrst stage of Sequencers involves patch\n",
      "\n",
      "of the proposed architectures.\n",
      "3.1 Preliminaries: Long short-term memory\n",
      "LSTM [27] is a specialized recurrent neural network (RNN) for modeling long-term dependencies of\n",
      "sequences. Plain LSTM has an input gate it that controls the storage of inputs, a forget gate ft that\n",
      "controls the forgetting of the former cell state ct−1 and an output gate ot that controls the cell output\n",
      "ht from the current cell state ct. Plain LSTM is formulated as follows:\n",
      "it = σ(Wxixt + Whiht−1 + bi) , ft = σ(Wxf xt + Whf ht−1 + bf ) , (1)\n",
      "ct = ft ⊙ct−1 + it ⊙tanh (Wxcxt + Whcht−1 + bc) , ot = σ(Wxoxt + Whoht−1 + bo) , (2)\n",
      "ht = ot ⊙tanh(ct), (3)\n",
      "where σis the logistic sigmoid function and ⊙is Hadamard product.\n",
      "3\n",
      "\n",
      "very different way than CNNs or ViTs. For more details on ERF and additional visualization, see\n",
      "Appendix D.\n",
      "Moreover we also visualized a hidden state of vertical and horizontal BiLSTM, and a feature map\n",
      "after channel fusion, and the results are visualized in Figure 4. It demonstrates that our Sequencer\n",
      "has the hidden states interact with each other over the vertical and horizontal directions. The closer\n",
      "tokens are in position, the stronger their interaction tends to be; the farther tokens are in position, the\n",
      "less their interaction tends to be.\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "context = build_context(relevant_chunks)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving\n",
      "Context is built from relevant chunks\n",
      "问题: What is lstm usef for?\n",
      "回答: LSTM (Long Short-Term Memory) is used in image recognition to process sequential data efficiently along both height and width, enabling better handling of complex patterns through bidirectional processing. This approach reduces computational complexity compared to other RNNs, making it suitable for tasks requiring understanding of local and global contexts in images.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def rag_pipeline(query: str, k: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    执行完整的RAG (检索增强生成) 流程。\n",
    "\n",
    "    参数:\n",
    "    query (str): 用户的问题\n",
    "    k (int): 要检索的文档块数量\n",
    "   \n",
    "\n",
    "    返回:\n",
    "    str: 生成的回答\n",
    "    \"\"\"\n",
    "    # 1. 检索相关文档\n",
    "    relevant_chunks = retrieve_related_context(query, db, k)\n",
    "    \n",
    "    # 2. 构建上下文\n",
    "    context = build_context(relevant_chunks)\n",
    "    \n",
    "    # 3. 生成回答\n",
    "    response = ollama_query(query, context)\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "# 测试完整的RAG系统\n",
    "test_query = \"What is lstm used for?\"\n",
    "response = rag_pipeline(test_query)\n",
    "print(f\"问题: {test_query}\")\n",
    "print(f\"回答: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
